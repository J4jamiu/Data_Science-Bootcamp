{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95e75006-9c3c-400a-b6fd-bce357a075ee",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7918c4b5-c767-4d75-8c84-67184622aa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f810c63-4f00-4edd-9b81-d234de4b2878",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ca2baab-9d8d-4625-b647-3de20b0ded4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92a64b9-a136-495b-b66e-6af3f0290fdd",
   "metadata": {},
   "source": [
    "DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bdf6e6a-eb3a-4751-ab0d-b906b4d6afa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\user\\tensorflow_datasets\\mnist\\3.0.1...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "894eee3f618a43699fbebf00a10689c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "248441073c594a63918802e766679586",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "534d3abf2b284df68a85a094217f0e57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction completed...: 0 file [00:00, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e07ff9ab50544ee39dadc989c9ad8c1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/2 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f522f1999ef42548df8bdef1c515b37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "140f8ac5992248b7882b054af0edddeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling C:\\Users\\user\\tensorflow_datasets\\mnist\\3.0.1.incomplete033BZ6\\mnist-train.tfrecord*...:   0%|      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e56dac4afc1d4aefaa8ec21010ab5bbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17100b924da940dfbab7f51638c1ce00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling C:\\Users\\user\\tensorflow_datasets\\mnist\\3.0.1.incomplete033BZ6\\mnist-test.tfrecord*...:   0%|       …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset mnist downloaded and prepared to C:\\Users\\user\\tensorflow_datasets\\mnist\\3.0.1. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8eaf21e-75f5-4d4d-95c4-511e9360128e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84c4d8af-3009-4ad1-aeb1-25ff76c12ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess the data – create a validation dataset and scale the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7533de77-4efc-495e-babf-05235f39b7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test'] = mnist_dataset['train'], mnist_dataset['test']\n",
    "num_validation_samples = 0.1*mnist_info.splits['train'].num_examples\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)\n",
    "\n",
    "def scale(image, label):\n",
    "    image = tf.cast(image,tf.float32)\n",
    "    image /=255.\n",
    "    return image, label\n",
    "scaled_train_and_validation_data = mnist_train.map(scale)\n",
    "test_data= mnist_test.map(scale)\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "test_data = test_data.batch(num_test_samples)\n",
    "\n",
    "validation_inputs, validation_targets = next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921ee3dd-fa0d-4b58-9d5a-87dd9c01290a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47418f51-57b9-4744-b944-8cc494182751",
   "metadata": {},
   "source": [
    "OUTLINE THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8911884-ffb8-42c7-92ba-83197907e7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_layer_size = 50\n",
    "model = tf.keras.Sequential([\n",
    "    \n",
    "                            tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
    "    \n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "                            tf.keras.layers.Dense(output_size, activation='softmax')\n",
    "                \n",
    "                            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "613a908d-0c2a-4c38-8eb6-a78b01ad5373",
   "metadata": {},
   "outputs": [],
   "source": [
    "#From the above, we have taken care of teh Data nad Model,we hve Obec tive function and OPTIMIZATION is "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023c19b8-725f-4363-8c89-b01598c68fe3",
   "metadata": {},
   "source": [
    "CHOOSE THE OPTIMIZER AND THE LOSS FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d74c8b0b-b2b0-4721-980f-8ca56eb2c5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We have 3 types of cross entropy: Binary, Categorical and sparse categorical entropy s\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a57b5e-a2bb-460a-add6-82e9d4197cd6",
   "metadata": {},
   "source": [
    "TRAIN THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aef5b30d-ebf2-4570-a601-d004459678e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 - 5s - loss: 0.4092 - accuracy: 0.8843 - val_loss: 0.2327 - val_accuracy: 0.9322 - 5s/epoch - 10ms/step\n",
      "Epoch 2/5\n",
      "540/540 - 2s - loss: 0.1927 - accuracy: 0.9431 - val_loss: 0.1810 - val_accuracy: 0.9467 - 2s/epoch - 4ms/step\n",
      "Epoch 3/5\n",
      "540/540 - 2s - loss: 0.1471 - accuracy: 0.9565 - val_loss: 0.1488 - val_accuracy: 0.9595 - 2s/epoch - 4ms/step\n",
      "Epoch 4/5\n",
      "540/540 - 3s - loss: 0.1181 - accuracy: 0.9648 - val_loss: 0.1116 - val_accuracy: 0.9688 - 3s/epoch - 5ms/step\n",
      "Epoch 5/5\n",
      "540/540 - 2s - loss: 0.0980 - accuracy: 0.9705 - val_loss: 0.1001 - val_accuracy: 0.9735 - 2s/epoch - 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1d347894df0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_EPOCHS = 5\n",
    "model.fit(train_data, epochs= NUM_EPOCHS,validation_data=(validation_inputs, validation_targets), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ecf36cc2-b1de-4630-89b1-907b0ecbbdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note that the accuracy we achieved at this stage is that of teh algorithim. we have to test the model to be sure othere is no overfitting or underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc209470-becf-435c-b84f-af842a5ae622",
   "metadata": {},
   "source": [
    "TEST THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f73a83d9-b772-4353-bd31-f7f0513030e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 563ms/step - loss: 0.1104 - accuracy: 0.9667\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0eee8dc8-4839-40fd-8cdd-a97dfa6b2e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test los: 0.11. Test accuracy: 96.67%\n"
     ]
    }
   ],
   "source": [
    "print('test los: {0:.2f}. Test accuracy: {1:.2f}%'. format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea2bba8-d94f-4a9b-b861-f715d850cb8c",
   "metadata": {},
   "source": [
    "From the above, it shows our model acuuracy is good as it is closed to the validation accuracy\n",
    "test accuracy is what we are expected to see if the model is applied to the real world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611d60ca-1c53-4992-8596-1e6133b42718",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
